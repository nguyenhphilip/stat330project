---
title: "Bayesian PCA for Adiantum Ecological Dataset"
author: "Morgan Southgate"
date: "November 10 2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---


# 0.1: Load packages
```{r echo=TRUE, message=FALSE, warning=FALSE}

# pca 
library(pcaMethods)
library(factoextra)
library(ade4)
library(Rdimtools)

## plotting
library(ggplot2)
library(ggfortify)
library(ggcorrplot)
library(ggrepel)
library(plotly)
library(cowplot)

# data management
library(reshape)
library(tidyr)
library(purrr)
library(plyr)
library(dplyr)

```

#0.2: Load Data
```{r}

micV <- read.csv("/home/adiantum/Documents/AdiantumResearch/SouthgateMorganPrivate/2_MicroEcologicalNiche/AdiantumData_FL2020.csv", header=T,sep=",",stringsAsFactors = F)

# head(micV)

```


#1: Data Management

```{r}
# eliminate categorical data and unnecessary ecological data
# colnames(micV)
micV <- micV%>%select(-FlowID,-SporeID,-StomateID,-PatchSize,-RamNum,-RamDens, -MicSlope,-MicAspect,-AdjMicAspect,-final_MicAspect,-MicHs,-MacAspect,-AdjMacAspect,-final_MacAspect,-MacHs,-HsPos,-DepSoil,-TexComp,-Ca.Mg)
## also eliminate Soil depth as variable because it doesn't really mean much in my data collection process. 

# ## eliminate absence plot, sterile hybrids
micV <- micV%>%filter(SpeciesFinal!="T")%>%filter(SpeciesFinal!="0")%>%filter(SpeciesFinal!="A?")%>%filter(SpeciesFinal!="V?")
unique(micV$SpeciesFinal)
```

## Correlation
```{r}
# get pearson correlation coefficients
# default is to compute correlation only for records with complete information.
# str(micV)
cortabR <- round(cor(micV[,12:40]),2)

# # Compute a matrix of correlation p-values
p.mat <- cor_pmat(micV[,12:40])
head(p.mat[, 1:4])

# correlation plot
# ggcorrplot(cortabR)

# reorder using hierarchical clustering
# ggcorrplot(cortabR, hc.order = TRUE, type="lower",outline.col = "white",lab=T,lab_size = 2)

# barring non-significant coefficient results from p.mat
ggcorrplot(cortabR, hc.order = TRUE, type="lower",outline.col = "white",p.mat=p.mat)


```

## select final vars
*Greater than 0.75?*
micro and macro slope strongly positively correlated
Ca and Ca_BS strongly positively correlated
Zn and Cd strongly positively correlated
K and K_BS strongly positively correlated
Ca_BS is strongly negatively correlated with both Mg and Mg_BS
ExchAcid and pH strongly negatively correlated

```{r}
micV <- micV%>%select(-Ca_BS,-Zn,-K_BS,-Mg_BS,-Ca_BS, -ExchAcid)
# str(micV)

```


#2: Data Distribution

## Melt data
```{r}
# raw data
micV_melt <- melt(micV,id.vars="SitePatch",measure.vars=colnames(micV[,12:35]))
head(micV_melt)
# str(micV_melt)
```

## plot individual vars
```{r}
# plot all at once
ggplot(micV_melt,aes(x=value))+
  geom_density()+
  facet_wrap(~variable,scales="free")

```


# 3: PCA
## 3.1 - Data Prep & Run

#### prep data
```{r}
# # reorganize species levels as factors

micV$SpeciesFinal <-factor(micV$SpeciesFinal,levels=c("A","A&V","V","V&P","P"))

# # reorganize state as factor
micV$State <- factor(micV$State,levels=c("PA","NY","NH","VT","ME","QC","QC_PNG"))

ad <- micV[,12:35]

```

#### run pca
```{r}
# run pca
# str(micV)
# colnames(micV)
ad.pca <- prcomp(micV[,c(12:35)],retx=T,center=T,scale.=T)

#View(ad.pca$rotation)

# write csv of loadings
write.csv(ad.pca$rotation[,1:5],"pca_loadings_1-3.csv")

summary(ad.pca)
# str(ad.pca$x)

# write coords for pc axes 1-3
pca_coords <- data.frame(micV$SitePatch,ad.pca$x[,1:5])

write.csv(pca_coords,"surv_plot_pca_coords.csv")

```

###  Set colors and variables
```{r}
# set colors and shapes
cols1 <- c("#E41A1C","#FF7F00","#4DAF4A","#377EB8","#984EA3")
shapes <- c(3,15,7,17,16,8,13)
Species <- micV$Species

```

## 3.2 - Plot
### 3d pca
```{r eval=FALSE, include=FALSE}
# recombine data in data frame
data.3d <- data.frame(micV[,1:11],ad.pca$x[,1:6])

# plot using plotly
plot_ly(data=data.3d,x=~PC1,y=~PC2,z=~PC4,color=~Species,size=1,type="scatter3d",mode="markers",colors=cols1,text=~SitePatch)
```


###  hulls
#### find hulls
```{r}
# # # find convex hulls 1 & 2
x <- data.frame(Species,ad.pca$x)
find_hull12 <- function(x) x[chull(x$PC1, x$PC2), ]
hulls12 <- ddply(x, "Species", find_hull12)

## find convext hulls 2 & 3
find_hull23 <- function(x) x[chull(x$PC2, x$PC3), ]
hulls23 <- ddply(x, "Species", find_hull23)

# modify figure to remove hulls for mixed survey plots
hulls12_mod <- hulls12%>%filter(Species!="A&V")%>%filter(Species!="V&P")
spp_hull <- hulls12_mod$Species
cols_hull <- c("#E41A1C","#4DAF4A","#984EA3")

# pc 2 and 3
hulls23_mod <- hulls23 %>% filter(Species!="A&V")%>%filter(Species!="V&P")
spp_hull1 <- hulls23_mod$Species
```

#### plot

```{r}
# PC 1 & 2 - plot with hulls 
ggplot(ad.pca,aes(PC1,PC2))+
  geom_point(aes(color=Species),size=2.25)+
  geom_polygon(hulls12_mod,mapping=aes(PC1,PC2,color=spp_hull,fill=spp_hull),alpha=0.2, show.legend=F)+
  scale_color_manual(values=cols1)+
  scale_fill_manual(values=cols_hull,guide="none")+
  theme_bw()+
  # scale_x_reverse()+
  # scale_y_reverse()+
  xlab("PC1 (20.28%)")+
  ylab("PC2 (15.38%)")+
  theme(plot.margin=unit(c(0,0.2,0,0.2),"cm"))+
  theme(axis.title=element_text(size=12))+
  theme(legend.title=element_text(size=14),legend.text=element_text(size=13))

# PC 2 & 3 - plot with hulls
ggplot(ad.pca,aes(PC2,PC3))+
  geom_point(aes(color=Species),size=2.25)+
  geom_polygon(hulls23_mod,mapping=aes(PC2,PC3,color=spp_hull1,fill=spp_hull1),alpha=0.2,show.legend=F)+
  scale_color_manual(values=cols1)+
  scale_fill_manual(values=cols_hull, guide="none")+
  theme_bw()+
  # scale_x_reverse()+
  # scale_y_reverse()+
  xlab("PC2 (15.38%)")+
  ylab("PC3 (10.25%)")+
  theme(plot.margin=unit(c(0,0.2,0,0.2),"cm"))+
   theme(axis.title=element_text(size=12))+
  theme(legend.title=element_text(size=14),legend.text=element_text(size=13))

```

#4 - Basic w/ factoextra
examples drawn from https://statweb.stanford.edu/~susan/courses/stats305c/examplesPPCA.html

## unscaled data
```{r}
# run pca
adi_pca <- dudi.pca(ad,scannf=FALSE,nf=10,scale=FALSE)
names(adi_pca)

# Scree plot
fviz_screeplot(adi_pca)

# proprtion variance explained in each component
round(adi_pca$eig,2)

# biplot!
fviz_pca_biplot(adi_pca)+coord_fixed()

#  how much of the total variation is explained by the first two components
sum(adi_pca$eig[1:2])/sum(adi_pca$eig)

```

## scaled analysis 
scales the correlation matrix 

```{r}
# run pca, this time scaling the matrix  - don't actually have to specify because it is the default
adi_pca2 <- dudi.pca(ad,scale=TRUE,scannf=FALSE,nf=10)

# scree plot
fviz_screeplot(adi_pca2)

# proportion of variance explained in each pc
round(adi_pca2$eig,1)

# biplot
fviz_pca_biplot(adi_pca2)+coord_fixed()

# graph of variables
fviz_pca_var(adi_pca2)

fviz_pca_ind(adi_pca2,habillage=Species)

# percentage of variance explained - 35% in first two components, pretty low - but this is ecological data
sum(adi_pca2$eig[1:2])/sum(adi_pca2$eig)
```

# 5 - pcaMethods
Benefits of this package seem to be that it provides a bunch of different scaling options for the data, along with the different types of pca. 

## prep data
```{r}
# pre=process matrix for PCA
# colnames(micV)
ad.c <- prep(micV[,12:35],center=T,simple=T,reverse=F)

listPcaMethods()
```

## SVD
```{r}
# scale specifies scaling based on unit variance
# cv names the type of cross-validation to be performed
ad.pca <- pca(ad.c,method="svd",scale="uv",nPcs=5,center=T,completeObs = F,cv="q2")

# look at the information contained in the pca object
slotNames(ad.pca)

# sdev for each prinicipal component
ad.pca@sDev

# variance for each principal component
ad.pca@sDev^2

# loadings and scores
dim(ad.pca@loadings) # 24 x 5
dim(ad.pca@scores) # 98 x 5

# specify variables
scores.pca <- data.frame(ad.pca@scores)
loadings.pca<- as.data.frame(ad.pca@loadings)
loadings.pca$Names <- rownames(loadings.pca)

# calculate Q^2 for cross-validation - used for estimating the level of structure in a data-set, and to optimize the choice of the number of principal components. 
# What is the possible range of the cross validation test (what counts as "low)
Q2(ad.pca,ad)

# loadings plot with ggplot2!
ggplot(loadings.pca)+
    geom_segment(data=loadings.pca, aes(x=0, y=0, xend=PC1, yend=PC2), arrow=arrow(length=unit(0.2,"cm")), alpha=0.25)+
    geom_text_repel(data=loadings.pca, aes(x=PC1, y=PC2, label=Names),  size=4)+
    scale_x_continuous("Principal Component 1")+
    scale_y_continuous("Principal Component 2")+
    coord_fixed()+ theme_bw()+ ggtitle("Standard PCA")

# plot of points on prinicipal components 1 and 2
ggplot(data=NULL,aes(x=scores.pca$PC1,y=scores.pca$PC2,color=Species))+
geom_point()+
  scale_color_manual(values=cols1)+
  theme_bw()+
  ggtitle("SVD PCA")
```


### function details
```{r eval=FALSE, include=FALSE}
function (object, method, nPcs = 2, scale = c("none", "pareto", 
    "vector", "uv"), center = TRUE, completeObs = TRUE, subset = NULL, 
    cv = c("none", "q2"), ...) 
{
    if (inherits(object, "data.frame")) {
        num <- vapply(object, is.numeric, logical(1))
        if (sum(num) < 2) 
            stop("no numeric data in supplied data.frame")
        Matrix <- as.matrix(object[, num])
    }
    else if (inherits(object, "ExpressionSet")) {
        Matrix <- t(exprs(object))
    }
    else Matrix <- as.matrix(object, rownames.force = TRUE)
    if (!is.null(subset)) 
        Matrix <- Matrix[, subset]
    cv <- match.arg(cv)
    scale <- match.arg(scale)
    if (nPcs > ncol(Matrix)) {
        warning("more components than matrix columns requested")
        nPcs <- min(dim(Matrix))
    }
    if (nPcs > nrow(Matrix)) {
        warning("more components than matrix rows requested")
        nPcs <- min(dim(Matrix))
    }
    if (!checkData(Matrix, verbose = interactive())) 
        stop("Invalid data format.", "Run checkData(data, verbose=TRUE) for details")
    missing <- is.na(Matrix)
    if (missing(method)) {
        if (any(missing)) 
            method <- "nipals"
        else method <- "svd"
    }
    if (any(missing) & method == "svd") {
        warning("data has missing values using nipals instead of user requested svd")
        method <- "nipals"
    }
    method <- match.arg(method, choices = listPcaMethods())
    prepres <- prep(Matrix, scale = scale, center = center, simple = FALSE, 
        ...)
    switch(method, svd = {
        res <- svdPca(prepres$data, nPcs = nPcs, ...)
    }, nipals = {
        res <- nipalsPca(prepres$data, nPcs = nPcs, ...)
    }, rnipals = {
        res <- RnipalsPca(prepres$data, nPcs = nPcs, ...)
    }, bpca = {
        res <- bpca(prepres$data, nPcs = nPcs, ...)
    }, ppca = {
        res <- ppca(prepres$data, nPcs = nPcs, ...)
    }, svdImpute = {
        res <- svdImpute(prepres$data, nPcs = nPcs, ...)
    }, robustPca = {
        res <- robustPca(prepres$data, nPcs = nPcs, ...)
    }, nlpca = {
        res <- nlpca(prepres$data, nPcs = nPcs, ...)
    })
    nPcs <- ncol(res@scores)
    if (is.null(scores(res)) | is.null(loadings(res)) | is.null(R2cum(res)) | 
        is.null(method(res))) 
        stop(paste("bad result from pca method", method))
    colnames(res@scores) <- paste("PC", 1:nPcs, sep = "")
    rownames(res@scores) <- rownames(Matrix)
    if (all(dim(loadings(res)) == c(ncol(Matrix), nPcs))) {
        colnames(res@loadings) <- paste("PC", 1:nPcs, sep = "")
        rownames(res@loadings) <- colnames(Matrix)
    }
    if (!is.null(subset)) 
        res@subset <- subset
    res@missing <- missing
    res@nPcs <- nPcs
    res@nObs <- nrow(Matrix)
    res@nVar <- ncol(Matrix)
    res@sDev <- apply(scores(res), 2, sd)
    res@center <- prepres$center
    res@centered <- center
    res@scale <- prepres$scale
    res@scaled <- scale
    res@R2 <- res@R2cum[1]
    if (length(res@R2cum) > 1) 
        res@R2 <- c(res@R2, diff(res@R2cum))
    if (completeObs) {
        cObs <- Matrix
        if (method %in% listPcaMethods("nonlinear")) 
            cObs[missing] <- fitted(res, Matrix, pre = TRUE, 
                post = TRUE)[missing]
        else cObs[missing] <- fitted(res, post = TRUE)[missing]
        res@completeObs <- cObs
    }
    if (cv == "q2") 
        res@cvstat <- Q2(res, Matrix, nruncv = 1, ...)
    return(res)
}

```

## PPCA

```{r}
# ppca
ad.ppca <- pca(ad.c,nPcs=5,method="ppca", scale=c("uv"))

# sdev for each prinicipal component
ad.ppca@sDev

# variance for each principal component
ad.ppca@sDev^2

# loadings and scores
dim(ad.ppca@loadings) # 24 x 5
dim(ad.ppca@scores) # 98 x 5

# specify variables
ppca.scores <- data.frame(ad.ppca@scores)
ppca.loadings<- as.data.frame(ad.ppca@loadings)
ppca.loadings$Names <- rownames(ppca.loadings)

# Q2 very low
Q2(ad.ppca,ad)

# loadings plot with ggplot2
ggplot(ppca.loadings)+
    geom_segment(data=ppca.loadings, aes(x=0, y=0, xend=PC1, yend=PC2), arrow=arrow(length=unit(0.2,"cm")), alpha=0.25)+
    geom_text_repel(data=ppca.loadings, aes(x=PC1, y=PC2, label=Names),  size=4)+
    scale_x_continuous("Principal Component 1")+
    scale_y_continuous("Principal Component 2")+
    coord_fixed()+ theme_bw()+ ggtitle("PPCA")

# plot of positions of points
ggplot(data=NULL,aes(x=ppca.scores$PC1,y=ppca.scores$PC2,color=Species))+
geom_point()+
  scale_color_manual(values=cols1)+
# stat_ellipse()+
  theme_bw()+
  scale_x_reverse()+
  ggtitle("PPCA")

```

### function details
```{r eval=FALSE, include=FALSE}
print(ppca)

function (Matrix, nPcs = 2, seed = NA, threshold = 1e-05, maxIterations = 1000, 
    ...) 
{
    if (!is.na(seed)) 
        set.seed(seed)
    N <- nrow(Matrix)
    D <- ncol(Matrix)
    Obs <- !is.na(Matrix)
    hidden <- which(is.na(Matrix))
    missing <- length(hidden)
    if (missing) {
        Matrix[hidden] <- 0
    }
    r <- sample(N)
    C <- t(Matrix[r[1:nPcs], , drop = FALSE])
    C <- matrix(rnorm(C), nrow(C), ncol(C), dimnames = labels(C))
    CtC <- t(C) %*% C
    X <- Matrix %*% C %*% solve(CtC)
    recon <- X %*% t(C)
    recon[hidden] <- 0
    ss <- sum(sum((recon - Matrix)^2))/(N * D - missing)
    count <- 1
    old <- Inf
    while (count > 0) {
        Sx <- solve(diag(nPcs) + CtC/ss)
        ss_old <- ss
        if (missing) {
            proj <- X %*% t(C)
            Matrix[hidden] <- proj[hidden]
        }
        X <- Matrix %*% C %*% Sx/ss
        SumXtX <- t(X) %*% X
        C <- (t(Matrix) %*% X) %*% solve((SumXtX + N * Sx))
        CtC <- t(C) %*% C
        ss <- (sum(sum((C %*% t(X) - t(Matrix))^2)) + N * sum(sum(CtC %*% 
            Sx)) + missing * ss_old)/(N * D)
        objective <- N * (D * log(ss) + sum(diag(Sx)) - log(det(Sx))) + sum(diag(SumXtX)) - missing * log(ss_old)
        rel_ch <- abs(1 - objective/old)
        old <- objective
        count <- count + 1
        if (rel_ch < threshold & count > 5) {
            count <- 0
        }
        else if (count > maxIterations) {
            count <- 0
            warning("stopped after max iterations, but rel_ch was > threshold")
        }
    }
    C <- orth(C)
    evs <- eigen(cov(Matrix %*% C))
    vals <- evs[[1]]
    vecs <- evs[[2]]
    C <- C %*% vecs
    X <- Matrix %*% C
    R2cum <- rep(NA, nPcs)
    TSS <- sum(Matrix^2, na.rm = TRUE)
    for (i in 1:ncol(C)) {
        difference <- Matrix - (X[, 1:i, drop = FALSE] %*% t(C[, 
            1:i, drop = FALSE]))
        R2cum[i] <- 1 - (sum(difference^2, na.rm = TRUE)/TSS)
    }
    res <- new("pcaRes")
    res@scores <- X
    res@loadings <- C
    res@R2cum <- R2cum
    res@method <- "ppca"
     return(res)
}
```


## BPCA

Why is result exactly the same as ppca and svd pca? Because the function is based on missing value estimation?

```{r}
ad.bpca <- pca(ad.c,method="bpca",scale="uv",nPcs=5,center=T,completeObs = T,cv="q2")

# sdev for each prinicipal component
ad.bpca@sDev

# variance for each principal component
ad.bpca@sDev^2

# loadings and scores
dim(ad.bpca@loadings) # 24 x 5
dim(ad.bpca@scores) # 98 x 5

# specify variables
bpca.scores <- data.frame(ad.bpca@scores)
bpca.loadings<- as.data.frame(ad.bpca@loadings)
bpca.loadings$Names <- rownames(bpca.loadings)

# Q2  - maybe these can be negative, and this isn't so bad?..
Q2(ad.bpca,ad)

# loadings plot with ggplot2
ggplot(bpca.loadings)+
    geom_segment(data=bpca.loadings, aes(x=0, y=0, xend=PC1, yend=PC2), arrow=arrow(length=unit(0.2,"cm")), alpha=0.25)+
    geom_text_repel(data=bpca.loadings, aes(x=PC1, y=PC2, label=Names),  size=4)+scale_x_reverse("Principal Component 1")+scale_y_reverse("Principal Component 2")+
    # scale_x_continuous("Principal Component 1")+
    # scale_y_continuous("Principal Component 2")+
    coord_fixed()+ theme_bw()+ ggtitle("BPCA")

# plot of positions of points
ggplot(data=NULL,aes(x=bpca.scores$PC1,y=bpca.scores$PC2,color=Species))+
geom_point()+
  scale_color_manual(values=cols1)+
  scale_x_reverse("Principal Componet 1")+
  scale_y_reverse()+
# stat_ellipse()+
  theme_bw()+
  ggtitle("BPCA")

```

### function details

The *BPCA_initmodel* function - not intended to be run separately from the bpca function. Calculates the initial eigenvectors by use of SVD from the complete rows. The data structure M is created and initial values are assigned. 
"Further elements are: galpha0, balpha0, alpha, gmu0, btau0, gtau0, SigW. These are working variables or constants."

The *BPCA_dostep* function - contains the actual implementation of the BPCA component estimation. Performs one step of the BPCA EM algorithm; it is called maxStep times from within the main loop in BPCAestimate.

#### bpca
```{r}
# print(bpca)

function (Matrix, nPcs = 2, maxSteps = 100, verbose = interactive(), 
    threshold = 1e-04, ...) 
{
    M <- BPCA_initmodel(Matrix, nPcs)
    tauold <- 1000
    for (step in 1:maxSteps) {
        M <- BPCA_dostep(M, Matrix)
        if (step%%10 == 0) {
            tau <- M$tau
            dtau <- abs(log10(tau) - log10(tauold))
            if (verbose) {
                cat("Step Number           : ", step, "\n")
                cat("Increase in precision : ", dtau, "\n")
                cat("----------", "\n")
            }
            if (dtau < threshold) {
                break
            }
            tauold <- tau
        }
    }
    R2cum <- rep(NA, nPcs)
    TSS <- sum(Matrix^2, na.rm = TRUE)
    for (i in 1:nPcs) {
        difference <- Matrix - (M$scores[, 1:i, drop = FALSE] %*% 
            t(M$PA[, 1:i, drop = FALSE]))
        R2cum[i] <- 1 - (sum(difference^2, na.rm = TRUE)/TSS)
    }
    result <- new("pcaRes")
    result@scores <- M$scores
    result@loadings <- M$PA
    result@R2cum <- R2cum
    result@method <- "bpca"
    return(result)
}

```


#### BPCA_initmodel()
https://rdrr.io/bioc/pcaMethods/src/R/BPCA_initmodel.R
```{r}
#Model initialization for Bayesian PCA. This function is NOT
##' inteded to be run separately!
##'
##' The function calculates the initial Eigenvectors by use of SVD
##' from the complete rows.  The data structure M is created and
##' initial values are  assigned.
##' @title Initialize BPCA model
##' @param y numeric matrix containing missing values. Missing values
##' are denoted as 'NA'
##' @param components Number of components used for estimation
##' @return List containing
##' \item{rows}{Row number of input matrix}
##' \item{cols}{Column number of input matrix}
##' \item{comps}{Number of components to use}
##' \item{yest}{(working variable) current estimate of complete data}
##' \item{row_miss}{(Array) Indizes of rows containing missing values}
##' \item{row_nomiss}{(Array) Indices of complete rows (such with no
##' missing values)}
##' \item{nans}{Matrix of same size as input data. TRUE if \code{input == NA},
##' false otherwise}
##' \item{mean}{Column wise data mean}
##' \item{PA}{ (d x k) Estimated principal axes (eigenvectors,
##' loadings) The matrix ROWS are the vectors}
##' \item{tau}{Estimated precision of the residual error}
##' \item{scores}{ Estimated scores}
##' Further elements are: galpha0, balpha0, alpha, gmu0, btau0, gtau0,
##' SigW. These are working variables or constants.
##' @author Wolfram Stacklies
BPCA_initmodel <- function(y, components) {
  ## Initialization, write static parameters to the central
  M <- NULL 
  M$rows <- nrow(y)
  M$cols <- ncol(y) 
  M$comps <- components ## Column number
  M$yest <- y ## Original data, NAs are set to 0 later on

  ## Find rows with missing values, etc...
  M$nans <- is.na(y)
  temp <- apply(M$nans, 1, sum)
  M$row_nomiss <- which(temp == 0)
  M$row_miss <- which(temp != 0)
  M$yest[M$nans] <- 0
  M$scores <- NULL

  ## Get the SVD of the complete rows
  covy <- cov(M$yest)
  values <- svd(covy, components, components)
  U <- values[[2]]
  S <- diag( values[[1]][1:components], nrow = components, ncol = components)
  V <- values[[3]]

  ## M$mean: column wise mean of the original data
  M$mean <- matrix(0, 1, M$cols)
  for(j in 1:M$cols) {
    idx <- which(!is.na(y[,j]))
    M$mean[j] <- mean(y[idx,j])
  }

  M$PA <- U %*% sqrt(S)
  M$tau <- 1 / ( sum(diag(covy)) - sum(diag(S)) )
  
  ## Constants etc
  taumax <- 1e10
  taumin <- 1e-10
  M$tau <- max( min(M$tau, taumax), taumin )

  M$galpha0 <- 1e-10
  M$balpha0 <- 1
  M$alpha <- (2 * M$galpha0 + M$cols) / (M$tau * diag(t(M$PA) %*% M$PA) + 2 * M$galpha0 / M$balpha0)

  M$gmu0 <- 0.001

  M$btau0 <- 1
  M$gtau0 <- 1e-10
  M$SigW <- diag(components)
  return(M)
}


```

####BPCA.dostep()
https://rdrr.io/github/hredestig/pcaMethods/src/R/BPCA_dostep.R
```{r}
BPCA_dostep <- function(M,y) {

  ## Empty matrix in which the scores are copied
  M$scores <- matrix(NA, M$rows, M$comps)

  ## Expectation step for data without missing values
  Rx <- diag(M$comps) + M$tau * t(M$PA) %*% M$PA + M$SigW
  Rxinv <- solve(Rx)
  idx <- M$row_nomiss

  if (length(idx) == 0) {
    trS <- 0
    T <- 0
  } else {
    dy <- y[idx,, drop=FALSE] - repmat(M$mean, length(idx), 1)
    x <- M$tau * Rxinv %*% t(M$PA) %*% t(dy)
    T <- t(dy) %*% t(x)
    trS <- sum(sum(dy * dy))

    ## Assign the scores for complete rows
    xTranspose <- t(x)
    for (i in 1:length(idx)) {
      M$scores[idx[i],] <- xTranspose[i,]
    }
  }
  ## Expectation step for incomplete data
  if( length(M$row_miss) > 0) {
    for(n in 1:length(M$row_miss)) {
      i  <- M$row_miss[n]
      dyo <- y[ i, !M$nans[i,], drop=FALSE] - M$mean[ !M$nans[i,], drop=FALSE]
      Wm <- M$PA[ M$nans[i,],, drop=FALSE]
      Wo <- M$PA[ !M$nans[i,],, drop=FALSE]
      Rxinv <- solve( (Rx - M$tau * t(Wm) %*% Wm))
      ex  <- M$tau * t(Wo) %*% t(dyo)
      x <- Rxinv %*% ex
      dym <- Wm %*% x
      dy <- y[i,, drop=FALSE]
      dy[ !M$nans[i,] ] <- t(dyo)
      dy[ M$nans[i,] ] <- t(dym)
      M$yest[i,] <- dy + M$mean
      T <- T + t(dy) %*% t(x)
      T[ M$nans[i,], ] <- T[ M$nans[i,],, drop=FALSE] + Wm %*% Rxinv
      trS <- trS + dy %*% t(dy) + sum(M$nans[i,]) / M$tau + 
        sum( diag(Wm %*% Rxinv %*% t(Wm)) ) 
      trS <- trS[1,1]
      ## Assign the scores for rows containing missing values
      M$scores[M$row_miss[n],] <- t(x)
    }
  }
  T <- T / M$rows
  trS <- trS / M$rows

  ## Maximation step
  Rxinv <- solve(Rx)
  Dw <- Rxinv + M$tau * t(T) %*% M$PA %*% Rxinv + 
    diag(M$alpha, nrow = length(M$alpha)) / M$rows
  Dwinv <- solve(Dw)
  M$PA <- T %*% Dwinv ## The new estimate of the principal axes (loadings)
  M$tau <- (M$cols + 2 * M$gtau0 / M$rows) / (trS - sum(diag(t(T) %*% M$PA)) +
                                              (M$mean %*% t(M$mean) * M$gmu0 + 2 * M$gtau0 / M$btau0) / M$rows)
  M$tau <- M$tau[1,1] ## convert to scalar
  M$SigW <- Dwinv * (M$cols / M$rows)
  M$alpha <- (2 * M$galpha0 + M$cols) / (M$tau * diag(t(M$PA) %*% M$PA) + 
                                         diag(M$SigW) + 2 * M$galpha0 / M$balpha0)

  return(M)
}
```

# 6 - Rdimtools
## bpca
```{r echo=TRUE}
library(Rdimtools)

ad.bpca2 <- do.bpca(ad.c,ndim=5)

# head(ad.bpca2$mp.W)

# the data points (scores)
dim(ad.bpca2$Y) # 98 x 5

head(ad.bpca2$Y)

ggplot(data=NULL,aes(x=ad.bpca2$Y[,1],y=ad.bpca2$Y[,2],color=Species))+
geom_point()+
  scale_color_manual(values=cols1)+
# stat_ellipse()+
  theme_bw()+
  ggtitle("Linear BPCA")


```

do.bpca
```{r}
# print(do.bpca)

function (X, ndim = 2, ...) 
{
    aux.typecheck(X)
    if ((!is.numeric(ndim)) || (ndim < 1) || (ndim >= ncol(X)) || 
        is.infinite(ndim) || is.na(ndim)) {
        stop("* do.bpca : 'ndim' is a positive integer in [1,#(covariates)).")
    }
    params = list(...)
    pnames = names(params)
    if ("reltol" %in% pnames) {
        reltol = max(.Machine$double.eps, as.double(params$reltol))
    }
    else {
        reltol = 10^(-4)
    }
    if ("maxiter" %in% pnames) {
        maxiter = max(5, round(params$maxiter))
    }
    else {
        maxiter = 100
    }
    rcppbpca = method_bpca(t(X), reltol, maxiter)
    smallidx = order(as.vector(rcppbpca$alpha))[1:ndim]
    mlsig2 = rcppbpca$sig2
    mlW = rcppbpca$W[, smallidx]
    M = (t(mlW) %*% mlW) + (diag(ncol(mlW)) * mlsig2)
    SOL = base::solve(M, t(mlW))
    projection = aux.adjprojection(t(SOL))
    result = list()
    result$Y = X %*% projection
    result$projection = projection
    result$mp.itercount = rcppbpca$itercount
    result$mp.sigma2 = rcppbpca$sig2
    result$mp.alpha = rcppbpca$alpha
    result$mp.W = rcppbpca$W
    result$algorithm = "linear:BPCA"
    return(structure(result, class = "Rdimtools"))
}
```

## do.lda

```{r}

ad.lda <- do.lda(ad.c,Species,ndim=3)

# the data points (scores)
dim(ad.lda$Y) # 98 x 5

head(ad.lda$Y)

ggplot(data=NULL,aes(x=ad.lda$Y[,1],y=ad.lda$Y[,2],color=Species))+
geom_point()+
  scale_color_manual(values=cols1)+
# stat_ellipse()+
  theme_bw()+
  ggtitle("Linear Discriminant Analysis")

```


